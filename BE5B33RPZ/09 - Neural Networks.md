# Neural Networks

In the perceptron algorithm we have two parameters $w\in \R^n$ and $b \in \R$ and then the classification $\hat y$ to two classes for a vector $x\in \R^n$ is performed as
$$
\hat y = \text{sign}(w \times x + b )
$$
Another way we can define the perceptron algorithm is:

<img src="Resources/09 - Neural Networks/image-20210112193815643.png" alt="image-20210112193815643" style="zoom:33%;" />

From here we can combine different perceptron classifiers to make a complex classifier by letting them use the same input and have their own outputs into other perceptron:

<img src="Resources/09 - Neural Networks/image-20210112194028157.png" alt="image-20210112194028157" style="zoom:33%;" />

Another string motivation for  forming such combinations of simple classifiers is the **Universal Approximation Theorem**. This theorem states that if $\sigma:\R \rarr \R$ is a non-constant, bounded and continuous function, and if $f$ is a continuous function on unity hypercube $[0,1]^n$, then for any $\epsilon > 0 $ there exists $N \in \N, v_i,b_i \in \R, w_i \in \R^n$ such that:
$$
F(x) = \sum_{i=1}^nv_i \sigma(w_i \times x + b_i)\\
|F(x) - f(x)| < \epsilon,~~ \forall x \in [0,1]^m
$$
We can see that the approximation is exactly captured by the following network:

<img src="Resources/09 - Neural Networks/image-20210112194453585.png" alt="image-20210112194453585" style="zoom:33%;" />

## Representation

Neural networks can be represented in layers. Here we have an example:

<img src="Resources/09 - Neural Networks/image-20210112194712096.png" alt="image-20210112194712096" style="zoom:33%;" />

But this can be rewritten more concisely as:

<img src="Resources/09 - Neural Networks/image-20210112194750023.png" alt="image-20210112194750023" style="zoom:33%;" />

We can keep adding more hidden layers and the network can be described as:

<img src="Resources/09 - Neural Networks/image-20210112194903165.png" alt="image-20210112194903165" style="zoom:33%;" />

### Hidden Layers

Each hidden layer is composed of an affine function, followed by non-linearity. There are several different functions we can use for this part:

- $\sigma(z) = \text{sign}(z)$: Used for the original perceptron, but it is not usable in neural nets because it is not differentiable at 0, and everywhere else the gradient it 0, so we can't optimize the parameters.
- $\sigma(z) = \tanh(z)$
- $\sigma(z) = \max(0,z)$
- $\sigma(z) = \max(0,z) + \min(9,sz)$  $(0 <s<1)$ 
- $\sigma(z) = \frac 1 {1+ e^{-z}}$ 

### Output Layers

In a **$K$-class classification** we normally label the classes from $1,\dots, k$ but that doesn't mean that elements from class $3$ are more similar to those in class $4$ than any other class. This is way it makes no sense to have directly a one dimensionally output $\hat y$ trying to estimate the correct label. To solve this, the output layer will produce a $K$-dimensional vector and then a SoftMax function will convert it to class probabilities:

<img src="Resources/09 - Neural Networks/image-20210113110956503.png" alt="image-20210113110956503" style="zoom:33%;" />

With:
$$
[\text{softmax}(z)]_k = \frac {e^{z_k}}{\sum_{l=1}^K e^{z_l}}
$$
For representing the class $y$ of a training data point $(x,y)$we use **one-hot** representation that creates a $K$-dimensional vector and all the probabilities are zero except for the $y^{th}$ place:
$$
\text{onehot}(y) = [\delta_{1y},\dots,\delta_{Ky}]^T = [0,\dots,1,\dots,0]^T \in \R^K
$$
The **loss function** of the output layer measures how far the prediction $\hat y \in [0,1]^K$ is from the target vector generated by the one-hot function. The loss function can be one of these functions:

- **Squared Difference**: This function will have 0 loss if the prediction matches the target but $>0$ otherwise. The function is:
  $$
  J(\hat y,y) = ||\hat y - y||^2
  $$

- **Negative lob-likelihood**:
  $$
  J(\hat y, y) = -y^T \log(\hat y) = - \sum_{i=1}^K y_i \log(\hat y_i) = -\log \hat y_l
  $$
  Where $l$ is the index of the target class of the training data point $(x,l)$.

## Training 

Let the training data be $\mathcal T = \{(x_1,y_1) ,\dots,(x_N,y_N)\}$ where we can assume that the labels  $y_i$'s have been  converted to their one-hot representation. Let $\theta$ represent all the parameters of the neural net, we want to minimize:
$$
J(\mathcal T;\theta)=\sum_{(x,y) \in \mathcal T}J(\hat y(x),y)
$$
To do this we are using gradient-based methods and minimizing with respect to $\theta$. We need to evaluate the gradient of loss with respect to the neural net parameters and the derivative of the loss function to use it for updates of the gradient-descent type:
$$
\theta_{t+1} \larr (\theta_t - \mu) \sum_{(x,t) \in \mathcal{T' \subseteq T}} \frac{\partial J(\hat y(x),y)}{\partial \theta} 
$$
where $\mu$ is the learning rate and we do not need to do this over the entire dataset.

### Computing the Gradient

When computing the gradient, we will make use of the chain rule. Let $f:\R^n\rarr R$ and $g:\R^m\rarr\R^n$ such that $f(g(x)) = f(y)$, it holds that:
$$
\frac{\partial f(g(\bold x)) }{\partial x_k} = \sum_{i=1}^n \frac{\partial f}{\partial y_i} \frac{\partial y_i}{\partial x_k}
$$
 We can rewrite this in matrix form as:
$$
\frac{\partial f(g(\bold x)) }{\partial \bold x} = \frac {\partial f}{\partial \bold y} \frac {\partial \bold y}{\partial \bold x} = f' \bold y'
$$
where $f' = \partial f / \partial \bold y$ and $\bold y' = \partial \bold y / \partial \bold x$  are the Jacobian matrices:
$$
f' = \left[\frac{\partial f}{\partial y_1},\dots,\frac{\partial f}{\partial y_n}\right] \\
\bold y' = 
\left[
\array{
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2}&\dots &\frac{\partial y_1}{\partial x_m}\\
\frac{\partial y_2}{\partial x_1} &\frac{\partial y_2}{\partial x_2} &\dots&\vdots\\
\vdots& \dots&\ddots&\vdots\\
\frac{\partial y_n}{\partial x_1} & \dots& \dots & \frac{\partial y_n}{\partial x_m}
}
\right]
$$

#### Example

We can calculate the gradient of the following neural net using back propagation:

<img src="Resources/09 - Neural Networks/image-20210113121116118.png" alt="image-20210113121116118" style="zoom:33%;" />

The set of parameters $\theta= \{W_1,W_2,W_3,b_1,b_2,b_3\}$. For the loss function we will sue squared difference.

1. First we need to compute $v_3 =\left. \frac {\partial J}{\partial \hat y}\right| _{\hat y}$, this is a row vector $\bold v_3 = 2(\hat y -y)^T\in \R^K$ 

2. Then we need to calculate the derivatives of $J$ with respect to $W_3$ and $b_3$:
   $$
   \frac{\partial J}{\partial b_3} = \bold v_3 \left.\frac{\partial J}{\partial b_3}\right|_{\bold a_3} = \bold v_3\\
   \frac{\partial J}{\partial (W_3)_{kl}} = \bold v_3 \frac{\partial \hat y}{\partial (W_3)_{kl}}  = \bold v_3 [0,\dots,(\bold a_3)_l,\dots,0]^T = (\bold v_3)_k (\bold a_3)_l
   $$
   Where in $[0,\dots,(a_3)_l,\dots,0]^T$ $(a_3)_l$ is at the $k^{th}$ position.

   Doing that process for all elements of the matrix $W_3$,we can obtain the following matrix:
   $$
   \frac{\partial J}{\partial W_3} = 
   \left [\array{
   \frac{\partial J}{\partial (W_3)_{11}} & \dots & \frac{\partial J}{\partial (W_3)_{1m_1}}\\
   \vdots& \ddots & \vdots
   \\
   \frac{\partial J}{\partial (W_3)_{K1}}& \dots & \frac{\partial J}{\partial (W_3)_{Km_1}}
   }\right] = \bold v_3 ^T \bold a_3^T
   $$
    

3. Now we are going to compute $\bold v_2$:
   $$
   \bold v_2 = \bold v_3 \left.\frac{\partial \hat y}{\partial \bold a_3} \right|_{\bold a_3}
    \left.\frac{\partial \bold a_3}{\partial \bold z_3} \right|_{\bold z_3}
    =
    \bold (v_3 W_3) \odot \sigma'(\bold z_2)
   $$
   Where $\sigma'(\bold x) = [\sigma'(x_1),\dots,\sigma'(x_n)]$ and $\sigma'$ is the derivative of $\sigma$. The symbol $\odot$ represent the element-wise product of the matrices.

4. We can now compute $W_2,b_2$ as:
   $$
   \frac{\partial J}{\partial b_2} =\bold v_2\\
   \frac{\partial J}{\partial W_2} = \bold v_2^T\bold a_2^T
   $$

5. For $\bold v_1$ we are repeating step 3:
   $$
   \bold v_1 = (\bold v_2 W_2) \odot \sigma'(\bold z_2)
   $$

6. And lastly we can get the parameters $W_1,b_2$ as:
   $$
   \frac{\partial J}{\partial b_1} =\bold v_1\\
   \frac{\partial J}{\partial W_1} = \bold v_1^T\bold a_1^T
   $$

## Deep Learning

Deep learning is a very successful branch of machine learning that has made a lot of progress in recent years. This field includes Convolutional Neural Nets (CNNs), where the inputs are translation-invariant and warpable, like in images. It also includes Recurrent Neural Networks (RNNs) which allow previous outputs to be  used as inputs and are very common in speech recognition. From now on we will only discus **CNNs**

The main advantage of CNNs is that they reduce the number of parameters needed. For example if we have a $64\times 64$ pixels image and we try to implement a NN with fully connected layers we end up with $(64^2)^2$ connections. If we use CNNs with a $5\times 5$ neighborhood for each neuron in the second layer and we make the parameters for all $5\times 5$ connections shared, we only need $25$ connections.

The typical structure for a CNN involves many convolutions, subsampling and finally some full connection layers to generate the labels:

<img src="Resources/09 - Neural Networks/image-20210113131441568.png" alt="image-20210113131441568" style="zoom:33%;" />

### Pooling

Pooling is a method to subsample our data and keep the important features. The idea is that we have to select a filter and then the stride. The filter will represent the area we want to output of and the stride will gives us the area of the input. A very used function for pooling is max pooling, which takes the higher number from each section of the input and returns in tin the output:

<img src="Resources/09 - Neural Networks/image-20210113131739047.png" alt="image-20210113131739047" style="zoom:33%;" />

### Dropout

Drop out increases significantly overfitting by removing neurons that are not useful for the classification.



