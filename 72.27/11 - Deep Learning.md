# Deep Learning

Learning is a model-free approach to determine the mapping between some input $x$ and some output $y$ by adjusting parameters. These free parameters are adjusted based on an optimization process. 

<img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.09.40.jpg" alt="Screen Shot 2022-06-15 at 19.09.40" style="zoom:50%;" />

Deep learning is a revolutionary technology to move ahead the digital boundary in:

- **Computer Vision**
- **Natural Language Processing**
- **Speech Recognition**

It specializes in representing data that have a hierarchic structure. The **deepness** of these networks implies that the number of jumps in the causal dependency is better. With internal layers providing a partial output of the network. 

These advancements are likely to be implemented in boring, careless or dangerous activities. Most successful deployments have either:

- A person somewhere in the loop.
- The cost of failure is very low

There are 4 base pillars of deep learning:

- **Multiple Levels of Composition**
- **Representational Learning**
- **GPU**
- **Massive and Standardized Datasets**

The **principle** of multiple realizability explains that:

> Different substrates can nonetheless perform the same input-output mapping but they are not the same

These models must have the ability to fit and to summarize.

## Deep Learning Problems

The following are some fundamental deep learning problems:

- Unsupervised pretraining
- Greedy Layer-wise
- LSTM cells
- Hessian free optimization
- Random weight guessing

## Shallow Networks

These neural networks are called **deep** because of limitations of ANN in real world problems.

Shallow networks are:

- Difficult to train
- Difficult to find the optimal result
- Difficult to generalize

Deep networks instead have a compositional structure that allows for shift invariance and locality at different scales.

## Deep Learning Models

We can differentiate the following deep-learning models:

- **Convolutional Neural Networks (CNN)**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.24.18.jpg" alt="Screen Shot 2022-06-15 at 19.24.18" style="zoom:50%;" />

- **Recurrent Neural Networks (RNN)**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.24.31.jpg" alt="Screen Shot 2022-06-15 at 19.24.31" style="zoom:50%;" />

- **Long Short Term Memory (LSTM)**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.25.12.jpg" alt="Screen Shot 2022-06-15 at 19.25.12" style="zoom:50%;" />

- **Deep Belief Networks (DBN)**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.25.54.jpg" alt="Screen Shot 2022-06-15 at 19.25.54" style="zoom:50%;" />

- **Autoencoders**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.26.36.jpg" alt="Screen Shot 2022-06-15 at 19.26.36" style="zoom:50%;" />

- **Generative Adverbial Networks (GAN)**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.28.41.jpg" alt="Screen Shot 2022-06-15 at 19.28.41" style="zoom:50%;" />

- **Transformers**

  <img src="Resources/11 - Deep Learning/Screen Shot 2022-06-15 at 19.27.12.jpg" alt="Screen Shot 2022-06-15 at 19.27.12" style="zoom:50%;" />





- **Biased Cherry Picking**: Selecting results that fit your claim.
- **Data Dredging**: Results of correlation was due to chance.
- **Survivorship bias**: Drawing conclusions from an incomplete set of data that survived. 
- **Perverse incentive**: the incentive produces the opposite result (wrong metrics, KPIs).
- **False causality**: two things occur together, they are related (falsely)
- **Garbage in, garbage out**.
- **Sampling bias** (cross validation, bootstrapping)
- **Gamblers fallacy**: believing that when something happen the chance of happening again is reduced.
- **Unbalanced Datasets**: medical images
- **Simpsons paradox**: a trend is present in a group but disappears when data is combined.























